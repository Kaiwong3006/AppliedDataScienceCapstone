I will first have to do a bit of data wrangling. There are a number of different non-integer values for columns, which means that I will first have to come up with numerical codes for certain conditions (such as road condition, weather, and type of address).
I will also need to deal with missing values for a number of the columns.  Several columns are missing thousands of values, which means that I will take the average of certain columns to filling in the missing values, and then I will drop others (or not use them).
Lastly, I will need to do some binning/category ranges for values such as X and Y. Given that I will probably use a Folium map to examine the accident data, I will need to come up with districts or other groups to categorize which areas are most dangerous. 
After this, I will split the revised data into training and test data. I will separate out the training values into two categories: 1 (Property damage) or 2 (Injury collision). 
From there, I will start by examining Pearson correlation with the SEVERITYCODE values to see if there is a good predictor for SEVERITYCODE 2. If nothing in particular stands out, I will try out different modeling techniques to see if there is a good fit for the training data. 
To make this easier, I will likely visualize the data in a number of different ways depending on what solution I am trying to pursue. Some examples will likely include Folium Maps, but also simple box plot/line graphs and scatter plots. 
From there, I will determine which factors I believe to be most relevant to the problem, and test it on the test data. I will examine the R^2/MSE to determine whether or not this is a good fit for hte data. 
